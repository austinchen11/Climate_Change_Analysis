# Data 

## Sources

The topic of our project - "Climate Change" - is by nature, broad; therefore, in order to answer the various questions of concern/interest, we had to incorporate many different data sources.  We will describe each of them below:

### Annual Surface Temperature Change
https://climatedata.imf.org/datasets/4063314923d74187be9596f10d034914/explore 
https://www.fao.org/faostat/en/#data/ET 

This data set showcases the annual estimates of mean surface temperature change from 1961 to 2021 by country, with respect to a baseline climatology (defined as the period of 1951 to 1980).  The Food and Agriculture Organization of the United Nations is the group that is responsible for compiling this data for public use; however, it seems like the FAO retrieves the data from elsewhere.  According to its website, the information comes from the publicly available GISTEMP data, managed by NASA-GISS.  With climate data, there are always a multitude of sources/options available, but I chose this specific website due to the accessible and downloadable CSV file, and the plethora of references it included.  The CSV file includes 227 different countries, each with time series data from 1961 to 2021.  Although there are other columns in the CSV, they are not relevant for our analysis.  There are two main issues with the data: missing values and messy format.  Both of these issues are easily dealt with using some data manipulation in R. 


### Atmospheric Carbon Dioxide Concentrations
https://climatedata.imf.org/datasets/9c3764c0efcc4c71934ab3988f219e0e/explore 
https://gml.noaa.gov/ccgg/trends/ 

This data set showcases the monthly atmospheric carbon dioxide levels, presented at the World level in parts per million (ppm). The data is collected by the Global Monitoring Laboratory of NOAA (specifically measured at Mauna Loa Observatory, Hawaii).  The data from this region actually constitutes the longest running direct measurement of carbon dioxide in the world.  There were other measures of carbon dioxide available, but since most climate analysis utilizes the data from Mauna Loa, I did not want to deviate from that.  The CSV file includes 1536 rows, with the world carbon dioxide concentration (in percent and parts per million) from 1958 to 2022.  There are other categorical variables in the data that are not necessary for our analysis.  The data itself is relatively clean; however, the format of file (like changing the dates into the correct format) will need to be slightly tweaked.  

### Change in Mean Sea Levels
https://climatedata.imf.org/datasets/b84a7e25159b4c65ba62d3f82c605855/explore 

This data set showcases the changes in mean sea levels across 24 regions of the world, and the specific measurements (in millimeters) are estimated from satellite radar altimeters.  The data is collected by the Laboratory for Satellite Altimetry from NOAA.  These estimates of sea level rise are based on measurements from satellite radar altimeters. These satellites "basically determine the distance from the satellite to a target surface by measuring the satellite-to-surface round-trip time of a radar pulse."  Like the previous data sources, I chose it due to the reputation of NOAA as an organization.  The CSV file has 35,379 rows, which encompasses the time series sea level change data from 1992 to 2022 for 24 regions around the world.  Other categorical variables exist, but are not necessary for our analysis. The data does not have any apparent issues, but the formatting (like with most data) will have to be tailored to our specific project.

### GDP and Population
https://stats.oecd.org/index.aspx?DataSetCode=PDB_LV

This data set showcases the level of GDP per capita and productivity, including GDP, total population, GDP per head of population, GDP per hour worked, GDP per person employed, etc. in different countries from 1970 to 2021. The data is collected by Organisation for Economic Co-operation and Development (OECD) and updated regularly. This data set can be downloaded as a csv file, containing 51,746 entries and 17 columns. It is well formatted and does not contain any null values, thus does not require data cleaning and preprocessing.

### Air Pollution
https://www.who.int/data/gho/data/themes/air-pollution/who-air-quality-database
https://www.iqair.com/us/world-most-polluted-countries

The first dataset showcases PM2.5, PM10, and NO2 concentration and temporal coverage from 2010 to 2018 in different countries and cities. The data is collected by WHO, and as described on the website, "the primary sources of data were official reports of countries sent to WHO upon request." It can be downloaded as an excel file and contains 32191 entries and 15 columns. This dataset contains null values and other irrelevant information, thus it requires cleaning and preprocessing.
Since we want to compare the air pollution before and after the rise of Covid-19, around 2019, we found another dataset which contains more up-to-date information. The second dataset from IQAir showcases PM2.5 concentration from 2018 to 2021 in 117 countries and contains 472 entries and 5 columns. The data cannot be downloaded, therefore, we choose to do web scraping and perform further cleaning and transformation. Finally, we combine these two datasets and do further analysis.


## Cleaning / transformation

Most of the data used in this project are Time Series data for different countries of the world; therefore, depending on the type of graph or type of merge/join we want to accomplish, the cleaning/transformation will differ.  We will denote some of them below:

### GDP and Population

The original dataset about GDP is from the United Nations, which showcases the GDP by country from 1970 to 2020. However, after plotting GDP values against year grouped by country, we found that the GDP of Japan and the US are two outliers, which have GDP much higher than other countries. Comparing this data with the other information we found, we concluded that this dataset contains wrong information and decided to search for other datasets.

The original dataset about population is from United Nations, containing total population and population growth in different countries in 2010, 2015, 2017, and 2022. After exploring the data, we realized that we need more datapoints in terms of year.

Therefore, we decided to use a new dataset from OECD which contains more comprehensive data, including the GDP, population, and other information in each country from 1970 to 2021.

### Air Pollution

The first air pollution dataset from WHO contains PM2.5 concentration in different cities. We drop all the null values and compute the average PM2.5 concentration among all the cities in each country in each year. We scrape the second dataset from IQAir website, and convert it from wide to long format.

We select PM2.5 concentration from countries with top 11 highest GDP and combine these two datasets for further analysis.


### Annual Surface Temperature Change

The temperature change data set included quite a bit of unnecessary categorical columns, therefore, as noted by the code snippet below, I removed them from the data frame.  Furthermore, missing values were simply omitted from the data set for a more complete analysis going forward.  In order to transform the data into a format that was easily graph-able, I used the function "melt" from the reshape2 library, which allowed for easier time series plotting.  Lastly, our group unanimously decided to only analyze the top 11 countries (based on GDP), since it is nearly impossible to evaluate 200+ countries of the world.

```{r}
temp_change = read.csv('data/Annual_Surface_Temperature_Change.csv', stringsAsFactors = TRUE)
temp_change = temp_change[, -c(3, 4, 5, 6 ,7 ,8 ,9, 10)]
temp_change = temp_change[, -c(1)]
temp_change_removed = na.omit(temp_change)

library(reshape2)

df_temp <- melt(subset(temp_change_removed, Country %in% c('United States', 'India', 'Japan', 'Germany', 'Indonesia', 'France', 'Brazil', 'United Kingdom', 'Italy', 'Mexico', 'China, P.R.: Mainland')), id.vars=c("Country"),value.name="Value", variable.name="Year")

df_temp_subset = subset(df_temp, Country %in% c('United States', 'India', 'Japan', 'Germany', 'Indonesia', 'France', 'Brazil', 'United Kingdom', 'Italy', 'Mexico', 'China, P.R.: Mainland'))

df_temp_subset$Year = substring(df_temp_subset$Year, 2)
```

### Atmospheric Carbon Dioxide Concentrations

The carbon dioxide data set did not require much transformation, as I only needed to remove some unnecessary columns and make sure all of the measurements were in parts per million.


```{r}

world_co2 = read.csv('data/Atmospheric_CO%E2%82%82_Concentrations.csv', stringsAsFactors = TRUE)
world_co2 = world_co2[, -c(1, 3, 4, 5, 7, 8, 9, 10)]
world_co2 = world_co2[world_co2$Unit == "Parts Per Million", ]
```

### Change in Mean Sea Levels

The sea level change data set required quite a bit of transformation before it was plot-ready.  First, all of the unnecessary categorical  variables were removed, and the "Date" column was formatted into a more traditional Date format.  Furthermore, multiple "group_by"s were conducted in order to output the mean sea level change of each calendar year for each of the regions included in the data set.  

```{r}
sea_level_change = read.csv("data/Change_in_Mean_Sea_Levels.csv", stringsAsFactors = TRUE)
sea_level_change = sea_level_change[, -c(1:10)]
sea_level_change$Date = substring(sea_level_change$Date, 2)
sea_level_change$Date = gsub("/", "-", sea_level_change$Date)
sea_level_change$Date = as.Date(sea_level_change$Date, format = "%m-%d-%Y")

sl_group = sea_level_change %>%
  group_by(Date) %>%
  summarize(mean_change = mean(Value))

sl_group_new = sl_group
sl_group_new$Date = substring(sl_group_new$Date, 1, 4)

sl_group_new_tidy = sl_group_new %>%
  group_by(Date) %>%
  summarise(average = mean(mean_change))

colnames(sl_group_new_tidy) = c("year", 'Average_SL_Change')
```

## Missing value analysis

Two of our data sets included some missing values; we will denote their analysis below.

### Annual Surface Temperature Change

```{r fig.width=6, fig.height=5}
colSums(is.na(temp_change))

library(mi)
x = missing_data.frame(temp_change)
image(x)
```

### Air Pollution
```{r}
library(openxlsx)
library(mi)
df_who_air_pollution <- read.xlsx("data/who_air_pollution.xlsx", sheet = 2)

x <- missing_data.frame(df_who_air_pollution)
image(x)
```

The air pollution data set from WHO contains many null values, specifically in 7 columns named PM2.5 (μg/m3), PM10 (μg/m3), NO2 (μg/m3), PM25 temporal coverage (%), PM10 temporal coverage (%), NO2 temporal coverage (%), Number and type of monitoring stations. We focus on PM2.5 (μg/m3) values. We drop the rows which are NAs in this column. Since we compute the average for each country, dropping null values has little impact on our results.








